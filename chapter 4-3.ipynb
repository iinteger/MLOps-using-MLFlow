{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "batch_size =256\n",
    "num_classes = 10\n",
    "learning_rate = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9912422 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5515259804d64e528f80fc880dbc6afd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/28881 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cd4593be2384361930f38f3d323f413"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1648877 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bd0eeeb4f234cce8090552ee34bca02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4542 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9da6d3e9cd0d4f38a5237536512e74f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=None)\n",
    "test_set = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "x_train, y_train = train_set.data, train_set.targets\n",
    "x_test, y_test = test_set.data, test_set.targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1], x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1], x_test.shape[2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def to_one_hot(num_classes, labels):\n",
    "    one_hot = torch.zeros(([labels.shape[0], num_classes]))\n",
    "\n",
    "    for f in range(len(labels)):\n",
    "        one_hot[f][labels[f]] = 1\n",
    "\n",
    "    return one_hot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "y_train = to_one_hot(num_classes, y_train)\n",
    "y_test = to_one_hot(num_classes, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(256, 128)\n",
    "        self.dense2 = nn.Linear(128, 64)\n",
    "        self.dense3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.flat1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dense2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dense3(x)\n",
    "        x = nn.Softmax()(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "model = model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(x_train, y_train)\n",
    "train_loader = data.DataLoader(dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\love9\\anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch_Num 0 Loss 0.3378298878669739\n",
      "Epoch 0 Batch_Num 1 Loss 0.3155236840248108\n",
      "Epoch 0 Batch_Num 2 Loss 0.30136531591415405\n",
      "Epoch 0 Batch_Num 3 Loss 0.27985188364982605\n",
      "Epoch 0 Batch_Num 4 Loss 0.26875343918800354\n",
      "Epoch 0 Batch_Num 5 Loss 0.24444004893302917\n",
      "Epoch 0 Batch_Num 6 Loss 0.2016797959804535\n",
      "Epoch 0 Batch_Num 7 Loss 0.1934983879327774\n",
      "Epoch 0 Batch_Num 8 Loss 0.16755257546901703\n",
      "Epoch 0 Batch_Num 9 Loss 0.15118944644927979\n",
      "Epoch 0 Batch_Num 10 Loss 0.1396394670009613\n",
      "Epoch 0 Batch_Num 11 Loss 0.11607567965984344\n",
      "Epoch 0 Batch_Num 12 Loss 0.13157711923122406\n",
      "Epoch 0 Batch_Num 13 Loss 0.1259670853614807\n",
      "Epoch 0 Batch_Num 14 Loss 0.09769929945468903\n",
      "Epoch 0 Batch_Num 15 Loss 0.10112454742193222\n",
      "Epoch 0 Batch_Num 16 Loss 0.10735225677490234\n",
      "Epoch 0 Batch_Num 17 Loss 0.08644517511129379\n",
      "Epoch 0 Batch_Num 18 Loss 0.10941453278064728\n",
      "Epoch 0 Batch_Num 19 Loss 0.09020671993494034\n",
      "Epoch 0 Batch_Num 20 Loss 0.08545426279306412\n",
      "Epoch 0 Batch_Num 21 Loss 0.06231990456581116\n",
      "Epoch 0 Batch_Num 22 Loss 0.10084204375743866\n",
      "Epoch 0 Batch_Num 23 Loss 0.05868327617645264\n",
      "Epoch 0 Batch_Num 24 Loss 0.05267874151468277\n",
      "Epoch 0 Batch_Num 25 Loss 0.07308079302310944\n",
      "Epoch 0 Batch_Num 26 Loss 0.07959286868572235\n",
      "Epoch 0 Batch_Num 27 Loss 0.10593406856060028\n",
      "Epoch 0 Batch_Num 28 Loss 0.09378205239772797\n",
      "Epoch 0 Batch_Num 29 Loss 0.059800803661346436\n",
      "Epoch 0 Batch_Num 30 Loss 0.08448561280965805\n",
      "Epoch 0 Batch_Num 31 Loss 0.07985522598028183\n",
      "Epoch 0 Batch_Num 32 Loss 0.07876993715763092\n",
      "Epoch 0 Batch_Num 33 Loss 0.08414644002914429\n",
      "Epoch 0 Batch_Num 34 Loss 0.1185067892074585\n",
      "Epoch 0 Batch_Num 35 Loss 0.056652672588825226\n",
      "Epoch 0 Batch_Num 36 Loss 0.07701223343610764\n",
      "Epoch 0 Batch_Num 37 Loss 0.05234319716691971\n",
      "Epoch 0 Batch_Num 38 Loss 0.05870402976870537\n",
      "Epoch 0 Batch_Num 39 Loss 0.0720963254570961\n",
      "Epoch 0 Batch_Num 40 Loss 0.062172163277864456\n",
      "Epoch 0 Batch_Num 41 Loss 0.05073181912302971\n",
      "Epoch 0 Batch_Num 42 Loss 0.049913473427295685\n",
      "Epoch 0 Batch_Num 43 Loss 0.06307362765073776\n",
      "Epoch 0 Batch_Num 44 Loss 0.04301384463906288\n",
      "Epoch 0 Batch_Num 45 Loss 0.09066691994667053\n",
      "Epoch 0 Batch_Num 46 Loss 0.053539253771305084\n",
      "Epoch 0 Batch_Num 47 Loss 0.04422817751765251\n",
      "Epoch 0 Batch_Num 48 Loss 0.062029093503952026\n",
      "Epoch 0 Batch_Num 49 Loss 0.08055688440799713\n",
      "Epoch 0 Batch_Num 50 Loss 0.06999218463897705\n",
      "Epoch 0 Batch_Num 51 Loss 0.07501466572284698\n",
      "Epoch 0 Batch_Num 52 Loss 0.04156721755862236\n",
      "Epoch 0 Batch_Num 53 Loss 0.047312069684267044\n",
      "Epoch 0 Batch_Num 54 Loss 0.06780824065208435\n",
      "Epoch 0 Batch_Num 55 Loss 0.0777430385351181\n",
      "Epoch 0 Batch_Num 56 Loss 0.07487998902797699\n",
      "Epoch 0 Batch_Num 57 Loss 0.08420035988092422\n",
      "Epoch 0 Batch_Num 58 Loss 0.04329969733953476\n",
      "Epoch 0 Batch_Num 59 Loss 0.05516298860311508\n",
      "Epoch 0 Batch_Num 60 Loss 0.04429489001631737\n",
      "Epoch 0 Batch_Num 61 Loss 0.057984836399555206\n",
      "Epoch 0 Batch_Num 62 Loss 0.05566699057817459\n",
      "Epoch 0 Batch_Num 63 Loss 0.034760501235723495\n",
      "Epoch 0 Batch_Num 64 Loss 0.03813554346561432\n",
      "Epoch 0 Batch_Num 65 Loss 0.05742473527789116\n",
      "Epoch 0 Batch_Num 66 Loss 0.05735883116722107\n",
      "Epoch 0 Batch_Num 67 Loss 0.039276205003261566\n",
      "Epoch 0 Batch_Num 68 Loss 0.061389803886413574\n",
      "Epoch 0 Batch_Num 69 Loss 0.04669364541769028\n",
      "Epoch 0 Batch_Num 70 Loss 0.03029167279601097\n",
      "Epoch 0 Batch_Num 71 Loss 0.034310005605220795\n",
      "Epoch 0 Batch_Num 72 Loss 0.041225336492061615\n",
      "Epoch 0 Batch_Num 73 Loss 0.029360618442296982\n",
      "Epoch 0 Batch_Num 74 Loss 0.0445832684636116\n",
      "Epoch 0 Batch_Num 75 Loss 0.03889773041009903\n",
      "Epoch 0 Batch_Num 76 Loss 0.03387226536870003\n",
      "Epoch 0 Batch_Num 77 Loss 0.034116230905056\n",
      "Epoch 0 Batch_Num 78 Loss 0.06358874589204788\n",
      "Epoch 0 Batch_Num 79 Loss 0.0340985506772995\n",
      "Epoch 0 Batch_Num 80 Loss 0.05729960277676582\n",
      "Epoch 0 Batch_Num 81 Loss 0.05070900917053223\n",
      "Epoch 0 Batch_Num 82 Loss 0.031274475157260895\n",
      "Epoch 0 Batch_Num 83 Loss 0.03335646167397499\n",
      "Epoch 0 Batch_Num 84 Loss 0.03153998404741287\n",
      "Epoch 0 Batch_Num 85 Loss 0.022950924932956696\n",
      "Epoch 0 Batch_Num 86 Loss 0.04929526895284653\n",
      "Epoch 0 Batch_Num 87 Loss 0.040040887892246246\n",
      "Epoch 0 Batch_Num 88 Loss 0.04414059966802597\n",
      "Epoch 0 Batch_Num 89 Loss 0.02896561287343502\n",
      "Epoch 0 Batch_Num 90 Loss 0.02624337002635002\n",
      "Epoch 0 Batch_Num 91 Loss 0.029627392068505287\n",
      "Epoch 0 Batch_Num 92 Loss 0.037539638578891754\n",
      "Epoch 0 Batch_Num 93 Loss 0.03660406917333603\n",
      "Epoch 0 Batch_Num 94 Loss 0.04320385679602623\n",
      "Epoch 0 Batch_Num 95 Loss 0.03196544200181961\n",
      "Epoch 0 Batch_Num 96 Loss 0.05979812145233154\n",
      "Epoch 0 Batch_Num 97 Loss 0.03226866200566292\n",
      "Epoch 0 Batch_Num 98 Loss 0.036532457917928696\n",
      "Epoch 0 Batch_Num 99 Loss 0.02321515791118145\n",
      "Epoch 0 Batch_Num 100 Loss 0.03744981437921524\n",
      "Epoch 0 Batch_Num 101 Loss 0.03123011253774166\n",
      "Epoch 0 Batch_Num 102 Loss 0.020703695714473724\n",
      "Epoch 0 Batch_Num 103 Loss 0.05407335236668587\n",
      "Epoch 0 Batch_Num 104 Loss 0.060428015887737274\n",
      "Epoch 0 Batch_Num 105 Loss 0.035167355090379715\n",
      "Epoch 0 Batch_Num 106 Loss 0.04494847357273102\n",
      "Epoch 0 Batch_Num 107 Loss 0.03606569766998291\n",
      "Epoch 0 Batch_Num 108 Loss 0.030291568487882614\n",
      "Epoch 0 Batch_Num 109 Loss 0.021086977794766426\n",
      "Epoch 0 Batch_Num 110 Loss 0.04384087398648262\n",
      "Epoch 0 Batch_Num 111 Loss 0.038263771682977676\n",
      "Epoch 0 Batch_Num 112 Loss 0.01936887577176094\n",
      "Epoch 0 Batch_Num 113 Loss 0.03645480424165726\n",
      "Epoch 0 Batch_Num 114 Loss 0.04464689642190933\n",
      "Epoch 0 Batch_Num 115 Loss 0.028455128893256187\n",
      "Epoch 0 Batch_Num 116 Loss 0.03472858667373657\n",
      "Epoch 0 Batch_Num 117 Loss 0.038413651287555695\n",
      "Epoch 0 Batch_Num 118 Loss 0.015866616740822792\n",
      "Epoch 0 Batch_Num 119 Loss 0.04864999279379845\n",
      "Epoch 0 Batch_Num 120 Loss 0.036137111485004425\n",
      "Epoch 0 Batch_Num 121 Loss 0.02996179834008217\n",
      "Epoch 0 Batch_Num 122 Loss 0.05105013772845268\n",
      "Epoch 0 Batch_Num 123 Loss 0.04033368453383446\n",
      "Epoch 0 Batch_Num 124 Loss 0.030109694227576256\n",
      "Epoch 0 Batch_Num 125 Loss 0.03360750526189804\n",
      "Epoch 0 Batch_Num 126 Loss 0.04953550547361374\n",
      "Epoch 0 Batch_Num 127 Loss 0.013302735984325409\n",
      "Epoch 0 Batch_Num 128 Loss 0.023520855233073235\n",
      "Epoch 0 Batch_Num 129 Loss 0.029109090566635132\n",
      "Epoch 0 Batch_Num 130 Loss 0.03560816869139671\n",
      "Epoch 0 Batch_Num 131 Loss 0.031112218275666237\n",
      "Epoch 0 Batch_Num 132 Loss 0.017824260517954826\n",
      "Epoch 0 Batch_Num 133 Loss 0.015556976199150085\n",
      "Epoch 0 Batch_Num 134 Loss 0.03582761064171791\n",
      "Epoch 0 Batch_Num 135 Loss 0.027521580457687378\n",
      "Epoch 0 Batch_Num 136 Loss 0.027243560180068016\n",
      "Epoch 0 Batch_Num 137 Loss 0.03158716484904289\n",
      "Epoch 0 Batch_Num 138 Loss 0.019914597272872925\n",
      "Epoch 0 Batch_Num 139 Loss 0.022110814228653908\n",
      "Epoch 0 Batch_Num 140 Loss 0.027751499786973\n",
      "Epoch 0 Batch_Num 141 Loss 0.02503269352018833\n",
      "Epoch 0 Batch_Num 142 Loss 0.03586714342236519\n",
      "Epoch 0 Batch_Num 143 Loss 0.023802217096090317\n",
      "Epoch 0 Batch_Num 144 Loss 0.030708130449056625\n",
      "Epoch 0 Batch_Num 145 Loss 0.03565223142504692\n",
      "Epoch 0 Batch_Num 146 Loss 0.05198904126882553\n",
      "Epoch 0 Batch_Num 147 Loss 0.040773432701826096\n",
      "Epoch 0 Batch_Num 148 Loss 0.022306814789772034\n",
      "Epoch 0 Batch_Num 149 Loss 0.022388959303498268\n",
      "Epoch 0 Batch_Num 150 Loss 0.03408972918987274\n",
      "Epoch 0 Batch_Num 151 Loss 0.021750714629888535\n",
      "Epoch 0 Batch_Num 152 Loss 0.015303807333111763\n",
      "Epoch 0 Batch_Num 153 Loss 0.043379709124565125\n",
      "Epoch 0 Batch_Num 154 Loss 0.03263280168175697\n",
      "Epoch 0 Batch_Num 155 Loss 0.023293903097510338\n",
      "Epoch 0 Batch_Num 156 Loss 0.014754965901374817\n",
      "Epoch 0 Batch_Num 157 Loss 0.022831890732049942\n",
      "Epoch 0 Batch_Num 158 Loss 0.020036175847053528\n",
      "Epoch 0 Batch_Num 159 Loss 0.02856566570699215\n",
      "Epoch 0 Batch_Num 160 Loss 0.02739318273961544\n",
      "Epoch 0 Batch_Num 161 Loss 0.0486334003508091\n",
      "Epoch 0 Batch_Num 162 Loss 0.022783346474170685\n",
      "Epoch 0 Batch_Num 163 Loss 0.02819925919175148\n",
      "Epoch 0 Batch_Num 164 Loss 0.03228234872221947\n",
      "Epoch 0 Batch_Num 165 Loss 0.032133445143699646\n",
      "Epoch 0 Batch_Num 166 Loss 0.028713684529066086\n",
      "Epoch 0 Batch_Num 167 Loss 0.033443331718444824\n",
      "Epoch 0 Batch_Num 168 Loss 0.02923032082617283\n",
      "Epoch 0 Batch_Num 169 Loss 0.015893373638391495\n",
      "Epoch 0 Batch_Num 170 Loss 0.018699711188673973\n",
      "Epoch 0 Batch_Num 171 Loss 0.03436928242444992\n",
      "Epoch 0 Batch_Num 172 Loss 0.03668593615293503\n",
      "Epoch 0 Batch_Num 173 Loss 0.03293600305914879\n",
      "Epoch 0 Batch_Num 174 Loss 0.015635041519999504\n",
      "Epoch 0 Batch_Num 175 Loss 0.05057042837142944\n",
      "Epoch 0 Batch_Num 176 Loss 0.02262807823717594\n",
      "Epoch 0 Batch_Num 177 Loss 0.02675890363752842\n",
      "Epoch 0 Batch_Num 178 Loss 0.03068515658378601\n",
      "Epoch 0 Batch_Num 179 Loss 0.03297349438071251\n",
      "Epoch 0 Batch_Num 180 Loss 0.04515177011489868\n",
      "Epoch 0 Batch_Num 181 Loss 0.02726297453045845\n",
      "Epoch 0 Batch_Num 182 Loss 0.02431677281856537\n",
      "Epoch 0 Batch_Num 183 Loss 0.02151806652545929\n",
      "Epoch 0 Batch_Num 184 Loss 0.03413635864853859\n",
      "Epoch 0 Batch_Num 185 Loss 0.03401780128479004\n",
      "Epoch 0 Batch_Num 186 Loss 0.02356802485883236\n",
      "Epoch 0 Batch_Num 187 Loss 0.02344011887907982\n",
      "Epoch 0 Batch_Num 188 Loss 0.013071976602077484\n",
      "Epoch 0 Batch_Num 189 Loss 0.014771699905395508\n",
      "Epoch 0 Batch_Num 190 Loss 0.008122825995087624\n",
      "Epoch 0 Batch_Num 191 Loss 0.04870245233178139\n",
      "Epoch 0 Batch_Num 192 Loss 0.021725615486502647\n",
      "Epoch 0 Batch_Num 193 Loss 0.03881709277629852\n",
      "Epoch 0 Batch_Num 194 Loss 0.02878066897392273\n",
      "Epoch 0 Batch_Num 195 Loss 0.0161601472645998\n",
      "Epoch 0 Batch_Num 196 Loss 0.04244093969464302\n",
      "Epoch 0 Batch_Num 197 Loss 0.022747552022337914\n",
      "Epoch 0 Batch_Num 198 Loss 0.027889177203178406\n",
      "Epoch 0 Batch_Num 199 Loss 0.013578581623733044\n",
      "Epoch 0 Batch_Num 200 Loss 0.021332530304789543\n",
      "Epoch 0 Batch_Num 201 Loss 0.0166909359395504\n",
      "Epoch 0 Batch_Num 202 Loss 0.01626996323466301\n",
      "Epoch 0 Batch_Num 203 Loss 0.03413322567939758\n",
      "Epoch 0 Batch_Num 204 Loss 0.015461171977221966\n",
      "Epoch 0 Batch_Num 205 Loss 0.017881054431200027\n",
      "Epoch 0 Batch_Num 206 Loss 0.054108876734972\n",
      "Epoch 0 Batch_Num 207 Loss 0.025064298883080482\n",
      "Epoch 0 Batch_Num 208 Loss 0.01372499018907547\n",
      "Epoch 0 Batch_Num 209 Loss 0.03767372667789459\n",
      "Epoch 0 Batch_Num 210 Loss 0.02889813482761383\n",
      "Epoch 0 Batch_Num 211 Loss 0.0166060458868742\n",
      "Epoch 0 Batch_Num 212 Loss 0.012773066759109497\n",
      "Epoch 0 Batch_Num 213 Loss 0.017230208963155746\n",
      "Epoch 0 Batch_Num 214 Loss 0.02468903362751007\n",
      "Epoch 0 Batch_Num 215 Loss 0.01780575141310692\n",
      "Epoch 0 Batch_Num 216 Loss 0.03444145619869232\n",
      "Epoch 0 Batch_Num 217 Loss 0.025902722030878067\n",
      "Epoch 0 Batch_Num 218 Loss 0.024410398676991463\n",
      "Epoch 0 Batch_Num 219 Loss 0.02089209482073784\n",
      "Epoch 0 Batch_Num 220 Loss 0.0184756089001894\n",
      "Epoch 0 Batch_Num 221 Loss 0.015244996175169945\n",
      "Epoch 0 Batch_Num 222 Loss 0.02268890291452408\n",
      "Epoch 0 Batch_Num 223 Loss 0.016680294647812843\n",
      "Epoch 0 Batch_Num 224 Loss 0.014522889629006386\n",
      "Epoch 0 Batch_Num 225 Loss 0.02889096364378929\n",
      "Epoch 0 Batch_Num 226 Loss 0.01504929643124342\n",
      "Epoch 0 Batch_Num 227 Loss 0.003269180189818144\n",
      "Epoch 0 Batch_Num 228 Loss 0.00860688742250204\n",
      "Epoch 0 Batch_Num 229 Loss 0.013680018484592438\n",
      "Epoch 0 Batch_Num 230 Loss 0.0014600020367652178\n",
      "Epoch 0 Batch_Num 231 Loss 0.011044436134397984\n",
      "Epoch 0 Batch_Num 232 Loss 0.007073166314512491\n",
      "Epoch 0 Batch_Num 233 Loss 0.030421003699302673\n",
      "Epoch 0 Batch_Num 234 Loss 0.034172698855400085\n",
      "Epoch 1 Batch_Num 0 Loss 0.025185251608490944\n",
      "Epoch 1 Batch_Num 1 Loss 0.01949884369969368\n",
      "Epoch 1 Batch_Num 2 Loss 0.014818087220191956\n",
      "Epoch 1 Batch_Num 3 Loss 0.03725254163146019\n",
      "Epoch 1 Batch_Num 4 Loss 0.031031768769025803\n",
      "Epoch 1 Batch_Num 5 Loss 0.01914311945438385\n",
      "Epoch 1 Batch_Num 6 Loss 0.014006802812218666\n",
      "Epoch 1 Batch_Num 7 Loss 0.02345648594200611\n",
      "Epoch 1 Batch_Num 8 Loss 0.020107988268136978\n",
      "Epoch 1 Batch_Num 9 Loss 0.010088630951941013\n",
      "Epoch 1 Batch_Num 10 Loss 0.02962445840239525\n",
      "Epoch 1 Batch_Num 11 Loss 0.019266171380877495\n",
      "Epoch 1 Batch_Num 12 Loss 0.008590935729444027\n",
      "Epoch 1 Batch_Num 13 Loss 0.013904986903071404\n",
      "Epoch 1 Batch_Num 14 Loss 0.020502854138612747\n",
      "Epoch 1 Batch_Num 15 Loss 0.012455937452614307\n",
      "Epoch 1 Batch_Num 16 Loss 0.013420583680272102\n",
      "Epoch 1 Batch_Num 17 Loss 0.014043750241398811\n",
      "Epoch 1 Batch_Num 18 Loss 0.016116907820105553\n",
      "Epoch 1 Batch_Num 19 Loss 0.023826224729418755\n",
      "Epoch 1 Batch_Num 20 Loss 0.02307846024632454\n",
      "Epoch 1 Batch_Num 21 Loss 0.011462025344371796\n",
      "Epoch 1 Batch_Num 22 Loss 0.028177713975310326\n",
      "Epoch 1 Batch_Num 23 Loss 0.017739571630954742\n",
      "Epoch 1 Batch_Num 24 Loss 0.016330286860466003\n",
      "Epoch 1 Batch_Num 25 Loss 0.011465497314929962\n",
      "Epoch 1 Batch_Num 26 Loss 0.02516544796526432\n",
      "Epoch 1 Batch_Num 27 Loss 0.018475933000445366\n",
      "Epoch 1 Batch_Num 28 Loss 0.021243223920464516\n",
      "Epoch 1 Batch_Num 29 Loss 0.014317676424980164\n",
      "Epoch 1 Batch_Num 30 Loss 0.024304291233420372\n",
      "Epoch 1 Batch_Num 31 Loss 0.016504090279340744\n",
      "Epoch 1 Batch_Num 32 Loss 0.027554383501410484\n",
      "Epoch 1 Batch_Num 33 Loss 0.02312386967241764\n",
      "Epoch 1 Batch_Num 34 Loss 0.040645018219947815\n",
      "Epoch 1 Batch_Num 35 Loss 0.012449718080461025\n",
      "Epoch 1 Batch_Num 36 Loss 0.02901807427406311\n",
      "Epoch 1 Batch_Num 37 Loss 0.013428394682705402\n",
      "Epoch 1 Batch_Num 38 Loss 0.008620521053671837\n",
      "Epoch 1 Batch_Num 39 Loss 0.02171803079545498\n",
      "Epoch 1 Batch_Num 40 Loss 0.016115743666887283\n",
      "Epoch 1 Batch_Num 41 Loss 0.009552912786602974\n",
      "Epoch 1 Batch_Num 42 Loss 0.01957305707037449\n",
      "Epoch 1 Batch_Num 43 Loss 0.012626354582607746\n",
      "Epoch 1 Batch_Num 44 Loss 0.02143220044672489\n",
      "Epoch 1 Batch_Num 45 Loss 0.04066065698862076\n",
      "Epoch 1 Batch_Num 46 Loss 0.01966448500752449\n",
      "Epoch 1 Batch_Num 47 Loss 0.01232235599309206\n",
      "Epoch 1 Batch_Num 48 Loss 0.015748370438814163\n",
      "Epoch 1 Batch_Num 49 Loss 0.022373542189598083\n",
      "Epoch 1 Batch_Num 50 Loss 0.018815884366631508\n",
      "Epoch 1 Batch_Num 51 Loss 0.025843584910035133\n",
      "Epoch 1 Batch_Num 52 Loss 0.020586103200912476\n",
      "Epoch 1 Batch_Num 53 Loss 0.01759323664009571\n",
      "Epoch 1 Batch_Num 54 Loss 0.017696306109428406\n",
      "Epoch 1 Batch_Num 55 Loss 0.024011792615056038\n",
      "Epoch 1 Batch_Num 56 Loss 0.026020467281341553\n",
      "Epoch 1 Batch_Num 57 Loss 0.023371953517198563\n",
      "Epoch 1 Batch_Num 58 Loss 0.008573760278522968\n",
      "Epoch 1 Batch_Num 59 Loss 0.019594823941588402\n",
      "Epoch 1 Batch_Num 60 Loss 0.01638769917190075\n",
      "Epoch 1 Batch_Num 61 Loss 0.020701590925455093\n",
      "Epoch 1 Batch_Num 62 Loss 0.01603878103196621\n",
      "Epoch 1 Batch_Num 63 Loss 0.013777012005448341\n",
      "Epoch 1 Batch_Num 64 Loss 0.01456479262560606\n",
      "Epoch 1 Batch_Num 65 Loss 0.020935053005814552\n",
      "Epoch 1 Batch_Num 66 Loss 0.013792437501251698\n",
      "Epoch 1 Batch_Num 67 Loss 0.014900237321853638\n",
      "Epoch 1 Batch_Num 68 Loss 0.02874821424484253\n",
      "Epoch 1 Batch_Num 69 Loss 0.021502623334527016\n",
      "Epoch 1 Batch_Num 70 Loss 0.015412440523505211\n",
      "Epoch 1 Batch_Num 71 Loss 0.011958028189837933\n",
      "Epoch 1 Batch_Num 72 Loss 0.011387844569981098\n",
      "Epoch 1 Batch_Num 73 Loss 0.009769496507942677\n",
      "Epoch 1 Batch_Num 74 Loss 0.01560478936880827\n",
      "Epoch 1 Batch_Num 75 Loss 0.019029226154088974\n",
      "Epoch 1 Batch_Num 76 Loss 0.015134821645915508\n",
      "Epoch 1 Batch_Num 77 Loss 0.01124655082821846\n",
      "Epoch 1 Batch_Num 78 Loss 0.021227937191724777\n",
      "Epoch 1 Batch_Num 79 Loss 0.012961183674633503\n",
      "Epoch 1 Batch_Num 80 Loss 0.021191280335187912\n",
      "Epoch 1 Batch_Num 81 Loss 0.019612431526184082\n",
      "Epoch 1 Batch_Num 82 Loss 0.010670221410691738\n",
      "Epoch 1 Batch_Num 83 Loss 0.012047305703163147\n",
      "Epoch 1 Batch_Num 84 Loss 0.015131558291614056\n",
      "Epoch 1 Batch_Num 85 Loss 0.010546141304075718\n",
      "Epoch 1 Batch_Num 86 Loss 0.015661027282476425\n",
      "Epoch 1 Batch_Num 87 Loss 0.009493047371506691\n",
      "Epoch 1 Batch_Num 88 Loss 0.02335938811302185\n",
      "Epoch 1 Batch_Num 89 Loss 0.00725990766659379\n",
      "Epoch 1 Batch_Num 90 Loss 0.010805859230458736\n",
      "Epoch 1 Batch_Num 91 Loss 0.009808076545596123\n",
      "Epoch 1 Batch_Num 92 Loss 0.019397681578993797\n",
      "Epoch 1 Batch_Num 93 Loss 0.018660198897123337\n",
      "Epoch 1 Batch_Num 94 Loss 0.01225644163787365\n",
      "Epoch 1 Batch_Num 95 Loss 0.01707354187965393\n",
      "Epoch 1 Batch_Num 96 Loss 0.027688447386026382\n",
      "Epoch 1 Batch_Num 97 Loss 0.014491376467049122\n",
      "Epoch 1 Batch_Num 98 Loss 0.015974218025803566\n",
      "Epoch 1 Batch_Num 99 Loss 0.010538856498897076\n",
      "Epoch 1 Batch_Num 100 Loss 0.009872433729469776\n",
      "Epoch 1 Batch_Num 101 Loss 0.015247632749378681\n",
      "Epoch 1 Batch_Num 102 Loss 0.006800138391554356\n",
      "Epoch 1 Batch_Num 103 Loss 0.03028055652976036\n",
      "Epoch 1 Batch_Num 104 Loss 0.02215075120329857\n",
      "Epoch 1 Batch_Num 105 Loss 0.014419838786125183\n",
      "Epoch 1 Batch_Num 106 Loss 0.02025619149208069\n",
      "Epoch 1 Batch_Num 107 Loss 0.018256021663546562\n",
      "Epoch 1 Batch_Num 108 Loss 0.016017118468880653\n",
      "Epoch 1 Batch_Num 109 Loss 0.009470684453845024\n",
      "Epoch 1 Batch_Num 110 Loss 0.025155136361718178\n",
      "Epoch 1 Batch_Num 111 Loss 0.021872712299227715\n",
      "Epoch 1 Batch_Num 112 Loss 0.01594056561589241\n",
      "Epoch 1 Batch_Num 113 Loss 0.015860144048929214\n",
      "Epoch 1 Batch_Num 114 Loss 0.01944693550467491\n",
      "Epoch 1 Batch_Num 115 Loss 0.017289601266384125\n",
      "Epoch 1 Batch_Num 116 Loss 0.019186625257134438\n",
      "Epoch 1 Batch_Num 117 Loss 0.018580468371510506\n",
      "Epoch 1 Batch_Num 118 Loss 0.005766848102211952\n",
      "Epoch 1 Batch_Num 119 Loss 0.020008018240332603\n",
      "Epoch 1 Batch_Num 120 Loss 0.011796491220593452\n",
      "Epoch 1 Batch_Num 121 Loss 0.01656649447977543\n",
      "Epoch 1 Batch_Num 122 Loss 0.024178095161914825\n",
      "Epoch 1 Batch_Num 123 Loss 0.020616929978132248\n",
      "Epoch 1 Batch_Num 124 Loss 0.014494307339191437\n",
      "Epoch 1 Batch_Num 125 Loss 0.016831830143928528\n",
      "Epoch 1 Batch_Num 126 Loss 0.033824775367975235\n",
      "Epoch 1 Batch_Num 127 Loss 0.004912248812615871\n",
      "Epoch 1 Batch_Num 128 Loss 0.016425449401140213\n",
      "Epoch 1 Batch_Num 129 Loss 0.009845159016549587\n",
      "Epoch 1 Batch_Num 130 Loss 0.021436171606183052\n",
      "Epoch 1 Batch_Num 131 Loss 0.01807764731347561\n",
      "Epoch 1 Batch_Num 132 Loss 0.009504428133368492\n",
      "Epoch 1 Batch_Num 133 Loss 0.009695791639387608\n",
      "Epoch 1 Batch_Num 134 Loss 0.02385980263352394\n",
      "Epoch 1 Batch_Num 135 Loss 0.03346976637840271\n",
      "Epoch 1 Batch_Num 136 Loss 0.017069293186068535\n",
      "Epoch 1 Batch_Num 137 Loss 0.018595198169350624\n",
      "Epoch 1 Batch_Num 138 Loss 0.010156923905014992\n",
      "Epoch 1 Batch_Num 139 Loss 0.014469739980995655\n",
      "Epoch 1 Batch_Num 140 Loss 0.019423063844442368\n",
      "Epoch 1 Batch_Num 141 Loss 0.017613135278224945\n",
      "Epoch 1 Batch_Num 142 Loss 0.023029154166579247\n",
      "Epoch 1 Batch_Num 143 Loss 0.017519427463412285\n",
      "Epoch 1 Batch_Num 144 Loss 0.016440793871879578\n",
      "Epoch 1 Batch_Num 145 Loss 0.010278410278260708\n",
      "Epoch 1 Batch_Num 146 Loss 0.02386758103966713\n",
      "Epoch 1 Batch_Num 147 Loss 0.02699849382042885\n",
      "Epoch 1 Batch_Num 148 Loss 0.01576758548617363\n",
      "Epoch 1 Batch_Num 149 Loss 0.008518499322235584\n",
      "Epoch 1 Batch_Num 150 Loss 0.018581567332148552\n",
      "Epoch 1 Batch_Num 151 Loss 0.009687177836894989\n",
      "Epoch 1 Batch_Num 152 Loss 0.008980844169855118\n",
      "Epoch 1 Batch_Num 153 Loss 0.03303275629878044\n",
      "Epoch 1 Batch_Num 154 Loss 0.01757848635315895\n",
      "Epoch 1 Batch_Num 155 Loss 0.018672477453947067\n",
      "Epoch 1 Batch_Num 156 Loss 0.00954461470246315\n",
      "Epoch 1 Batch_Num 157 Loss 0.01585599221289158\n",
      "Epoch 1 Batch_Num 158 Loss 0.009627306833863258\n",
      "Epoch 1 Batch_Num 159 Loss 0.008183580823242664\n",
      "Epoch 1 Batch_Num 160 Loss 0.014949900098145008\n",
      "Epoch 1 Batch_Num 161 Loss 0.026486262679100037\n",
      "Epoch 1 Batch_Num 162 Loss 0.014816692098975182\n",
      "Epoch 1 Batch_Num 163 Loss 0.01428929902613163\n",
      "Epoch 1 Batch_Num 164 Loss 0.01372537948191166\n",
      "Epoch 1 Batch_Num 165 Loss 0.014740297570824623\n",
      "Epoch 1 Batch_Num 166 Loss 0.014032979495823383\n",
      "Epoch 1 Batch_Num 167 Loss 0.026035383343696594\n",
      "Epoch 1 Batch_Num 168 Loss 0.0187070332467556\n",
      "Epoch 1 Batch_Num 169 Loss 0.008386950939893723\n",
      "Epoch 1 Batch_Num 170 Loss 0.013988368213176727\n",
      "Epoch 1 Batch_Num 171 Loss 0.023594584316015244\n",
      "Epoch 1 Batch_Num 172 Loss 0.018424328416585922\n",
      "Epoch 1 Batch_Num 173 Loss 0.023373164236545563\n",
      "Epoch 1 Batch_Num 174 Loss 0.008003181777894497\n",
      "Epoch 1 Batch_Num 175 Loss 0.01908819004893303\n",
      "Epoch 1 Batch_Num 176 Loss 0.010019886307418346\n",
      "Epoch 1 Batch_Num 177 Loss 0.014206652529537678\n",
      "Epoch 1 Batch_Num 178 Loss 0.013746838085353374\n",
      "Epoch 1 Batch_Num 179 Loss 0.02379903383553028\n",
      "Epoch 1 Batch_Num 180 Loss 0.026512552052736282\n",
      "Epoch 1 Batch_Num 181 Loss 0.012849914841353893\n",
      "Epoch 1 Batch_Num 182 Loss 0.013809898868203163\n",
      "Epoch 1 Batch_Num 183 Loss 0.01885899342596531\n",
      "Epoch 1 Batch_Num 184 Loss 0.022178394719958305\n",
      "Epoch 1 Batch_Num 185 Loss 0.025142570957541466\n",
      "Epoch 1 Batch_Num 186 Loss 0.01415156852453947\n",
      "Epoch 1 Batch_Num 187 Loss 0.012273128144443035\n",
      "Epoch 1 Batch_Num 188 Loss 0.00899246521294117\n",
      "Epoch 1 Batch_Num 189 Loss 0.0048501440323889256\n",
      "Epoch 1 Batch_Num 190 Loss 0.007241274230182171\n",
      "Epoch 1 Batch_Num 191 Loss 0.029377907514572144\n",
      "Epoch 1 Batch_Num 192 Loss 0.011913711205124855\n",
      "Epoch 1 Batch_Num 193 Loss 0.024333873763680458\n",
      "Epoch 1 Batch_Num 194 Loss 0.018001852557063103\n",
      "Epoch 1 Batch_Num 195 Loss 0.007348243147134781\n",
      "Epoch 1 Batch_Num 196 Loss 0.02319970726966858\n",
      "Epoch 1 Batch_Num 197 Loss 0.00884490180760622\n",
      "Epoch 1 Batch_Num 198 Loss 0.022790737450122833\n",
      "Epoch 1 Batch_Num 199 Loss 0.008310841396450996\n",
      "Epoch 1 Batch_Num 200 Loss 0.00999403279274702\n",
      "Epoch 1 Batch_Num 201 Loss 0.008220207877457142\n",
      "Epoch 1 Batch_Num 202 Loss 0.011363597586750984\n",
      "Epoch 1 Batch_Num 203 Loss 0.02729244902729988\n",
      "Epoch 1 Batch_Num 204 Loss 0.014373702928423882\n",
      "Epoch 1 Batch_Num 205 Loss 0.009334228001534939\n",
      "Epoch 1 Batch_Num 206 Loss 0.03445705026388168\n",
      "Epoch 1 Batch_Num 207 Loss 0.017291288822889328\n",
      "Epoch 1 Batch_Num 208 Loss 0.009973777458071709\n",
      "Epoch 1 Batch_Num 209 Loss 0.0230710506439209\n",
      "Epoch 1 Batch_Num 210 Loss 0.016869988292455673\n",
      "Epoch 1 Batch_Num 211 Loss 0.020082280039787292\n",
      "Epoch 1 Batch_Num 212 Loss 0.007997742854058743\n",
      "Epoch 1 Batch_Num 213 Loss 0.010983705520629883\n",
      "Epoch 1 Batch_Num 214 Loss 0.011638620868325233\n",
      "Epoch 1 Batch_Num 215 Loss 0.009788870811462402\n",
      "Epoch 1 Batch_Num 216 Loss 0.021172812208533287\n",
      "Epoch 1 Batch_Num 217 Loss 0.017224203795194626\n",
      "Epoch 1 Batch_Num 218 Loss 0.020084058865904808\n",
      "Epoch 1 Batch_Num 219 Loss 0.013977664522826672\n",
      "Epoch 1 Batch_Num 220 Loss 0.011658747680485249\n",
      "Epoch 1 Batch_Num 221 Loss 0.010058403015136719\n",
      "Epoch 1 Batch_Num 222 Loss 0.010473419912159443\n",
      "Epoch 1 Batch_Num 223 Loss 0.010144177824258804\n",
      "Epoch 1 Batch_Num 224 Loss 0.012446925044059753\n",
      "Epoch 1 Batch_Num 225 Loss 0.018134061247110367\n",
      "Epoch 1 Batch_Num 226 Loss 0.013452415354549885\n",
      "Epoch 1 Batch_Num 227 Loss 0.001517565455287695\n",
      "Epoch 1 Batch_Num 228 Loss 0.004264225717633963\n",
      "Epoch 1 Batch_Num 229 Loss 0.00951327197253704\n",
      "Epoch 1 Batch_Num 230 Loss 0.0007316472474485636\n",
      "Epoch 1 Batch_Num 231 Loss 0.006994947791099548\n",
      "Epoch 1 Batch_Num 232 Loss 0.005641338415443897\n",
      "Epoch 1 Batch_Num 233 Loss 0.025395328179001808\n",
      "Epoch 1 Batch_Num 234 Loss 0.029327338561415672\n",
      "Epoch 2 Batch_Num 0 Loss 0.020921215415000916\n",
      "Epoch 2 Batch_Num 1 Loss 0.01268303394317627\n",
      "Epoch 2 Batch_Num 2 Loss 0.012149275280535221\n",
      "Epoch 2 Batch_Num 3 Loss 0.022914018481969833\n",
      "Epoch 2 Batch_Num 4 Loss 0.016214145347476006\n",
      "Epoch 2 Batch_Num 5 Loss 0.017037808895111084\n",
      "Epoch 2 Batch_Num 6 Loss 0.011081474833190441\n",
      "Epoch 2 Batch_Num 7 Loss 0.007571680936962366\n",
      "Epoch 2 Batch_Num 8 Loss 0.008248835802078247\n",
      "Epoch 2 Batch_Num 9 Loss 0.005878056865185499\n",
      "Epoch 2 Batch_Num 10 Loss 0.017774002626538277\n",
      "Epoch 2 Batch_Num 11 Loss 0.013292980380356312\n",
      "Epoch 2 Batch_Num 12 Loss 0.004371936898678541\n",
      "Epoch 2 Batch_Num 13 Loss 0.011661481112241745\n",
      "Epoch 2 Batch_Num 14 Loss 0.012378744781017303\n",
      "Epoch 2 Batch_Num 15 Loss 0.007208468858152628\n",
      "Epoch 2 Batch_Num 16 Loss 0.0039677401073277\n",
      "Epoch 2 Batch_Num 17 Loss 0.007538958452641964\n",
      "Epoch 2 Batch_Num 18 Loss 0.008203543722629547\n",
      "Epoch 2 Batch_Num 19 Loss 0.0193746667355299\n",
      "Epoch 2 Batch_Num 20 Loss 0.016684910282492638\n",
      "Epoch 2 Batch_Num 21 Loss 0.005280974321067333\n",
      "Epoch 2 Batch_Num 22 Loss 0.015414705500006676\n",
      "Epoch 2 Batch_Num 23 Loss 0.0136489886790514\n",
      "Epoch 2 Batch_Num 24 Loss 0.012588972225785255\n",
      "Epoch 2 Batch_Num 25 Loss 0.010211871936917305\n",
      "Epoch 2 Batch_Num 26 Loss 0.016906986013054848\n",
      "Epoch 2 Batch_Num 27 Loss 0.013986063189804554\n",
      "Epoch 2 Batch_Num 28 Loss 0.01289038173854351\n",
      "Epoch 2 Batch_Num 29 Loss 0.009656843729317188\n",
      "Epoch 2 Batch_Num 30 Loss 0.01659473404288292\n",
      "Epoch 2 Batch_Num 31 Loss 0.011348527856171131\n",
      "Epoch 2 Batch_Num 32 Loss 0.020780134946107864\n",
      "Epoch 2 Batch_Num 33 Loss 0.010180179961025715\n",
      "Epoch 2 Batch_Num 34 Loss 0.023205121979117393\n",
      "Epoch 2 Batch_Num 35 Loss 0.007070106919854879\n",
      "Epoch 2 Batch_Num 36 Loss 0.02107984945178032\n",
      "Epoch 2 Batch_Num 37 Loss 0.011118262074887753\n",
      "Epoch 2 Batch_Num 38 Loss 0.005706948693841696\n",
      "Epoch 2 Batch_Num 39 Loss 0.012909917160868645\n",
      "Epoch 2 Batch_Num 40 Loss 0.012484930455684662\n",
      "Epoch 2 Batch_Num 41 Loss 0.00510561466217041\n",
      "Epoch 2 Batch_Num 42 Loss 0.015501657500863075\n",
      "Epoch 2 Batch_Num 43 Loss 0.013269392773509026\n",
      "Epoch 2 Batch_Num 44 Loss 0.009210968390107155\n",
      "Epoch 2 Batch_Num 45 Loss 0.01956496573984623\n",
      "Epoch 2 Batch_Num 46 Loss 0.01826469786465168\n",
      "Epoch 2 Batch_Num 47 Loss 0.009397720918059349\n",
      "Epoch 2 Batch_Num 48 Loss 0.011772079393267632\n",
      "Epoch 2 Batch_Num 49 Loss 0.01345158088952303\n",
      "Epoch 2 Batch_Num 50 Loss 0.008763773366808891\n",
      "Epoch 2 Batch_Num 51 Loss 0.01836555264890194\n",
      "Epoch 2 Batch_Num 52 Loss 0.013553263619542122\n",
      "Epoch 2 Batch_Num 53 Loss 0.014564231038093567\n",
      "Epoch 2 Batch_Num 54 Loss 0.014786508865654469\n",
      "Epoch 2 Batch_Num 55 Loss 0.01756248250603676\n",
      "Epoch 2 Batch_Num 56 Loss 0.02054614946246147\n",
      "Epoch 2 Batch_Num 57 Loss 0.012575114145874977\n",
      "Epoch 2 Batch_Num 58 Loss 0.0063519529066979885\n",
      "Epoch 2 Batch_Num 59 Loss 0.011575570330023766\n",
      "Epoch 2 Batch_Num 60 Loss 0.00607479689642787\n",
      "Epoch 2 Batch_Num 61 Loss 0.014890506863594055\n",
      "Epoch 2 Batch_Num 62 Loss 0.013033263385295868\n",
      "Epoch 2 Batch_Num 63 Loss 0.01294744573533535\n",
      "Epoch 2 Batch_Num 64 Loss 0.01754789985716343\n",
      "Epoch 2 Batch_Num 65 Loss 0.01643790677189827\n",
      "Epoch 2 Batch_Num 66 Loss 0.014117008075118065\n",
      "Epoch 2 Batch_Num 67 Loss 0.010122252628207207\n",
      "Epoch 2 Batch_Num 68 Loss 0.009781988337635994\n",
      "Epoch 2 Batch_Num 69 Loss 0.013601414859294891\n",
      "Epoch 2 Batch_Num 70 Loss 0.008118429221212864\n",
      "Epoch 2 Batch_Num 71 Loss 0.014115932397544384\n",
      "Epoch 2 Batch_Num 72 Loss 0.00413919985294342\n",
      "Epoch 2 Batch_Num 73 Loss 0.00684871431440115\n",
      "Epoch 2 Batch_Num 74 Loss 0.014454762451350689\n",
      "Epoch 2 Batch_Num 75 Loss 0.014308787882328033\n",
      "Epoch 2 Batch_Num 76 Loss 0.008806934580206871\n",
      "Epoch 2 Batch_Num 77 Loss 0.004837856628000736\n",
      "Epoch 2 Batch_Num 78 Loss 0.01811237633228302\n",
      "Epoch 2 Batch_Num 79 Loss 0.008957231417298317\n",
      "Epoch 2 Batch_Num 80 Loss 0.016549214720726013\n",
      "Epoch 2 Batch_Num 81 Loss 0.009802969172596931\n",
      "Epoch 2 Batch_Num 82 Loss 0.006460112985223532\n",
      "Epoch 2 Batch_Num 83 Loss 0.005767747759819031\n",
      "Epoch 2 Batch_Num 84 Loss 0.011626673862338066\n",
      "Epoch 2 Batch_Num 85 Loss 0.007345191203057766\n",
      "Epoch 2 Batch_Num 86 Loss 0.011263052001595497\n",
      "Epoch 2 Batch_Num 87 Loss 0.005292579997330904\n",
      "Epoch 2 Batch_Num 88 Loss 0.012955114245414734\n",
      "Epoch 2 Batch_Num 89 Loss 0.0039942204020917416\n",
      "Epoch 2 Batch_Num 90 Loss 0.009876476600766182\n",
      "Epoch 2 Batch_Num 91 Loss 0.006183730438351631\n",
      "Epoch 2 Batch_Num 92 Loss 0.009786104783415794\n",
      "Epoch 2 Batch_Num 93 Loss 0.017267616465687752\n",
      "Epoch 2 Batch_Num 94 Loss 0.01267070509493351\n",
      "Epoch 2 Batch_Num 95 Loss 0.00907229445874691\n",
      "Epoch 2 Batch_Num 96 Loss 0.021989550441503525\n",
      "Epoch 2 Batch_Num 97 Loss 0.009530109353363514\n",
      "Epoch 2 Batch_Num 98 Loss 0.011663662269711494\n",
      "Epoch 2 Batch_Num 99 Loss 0.009717126376926899\n",
      "Epoch 2 Batch_Num 100 Loss 0.007434667553752661\n",
      "Epoch 2 Batch_Num 101 Loss 0.006320769898593426\n",
      "Epoch 2 Batch_Num 102 Loss 0.004844111856073141\n",
      "Epoch 2 Batch_Num 103 Loss 0.02201535925269127\n",
      "Epoch 2 Batch_Num 104 Loss 0.019852090626955032\n",
      "Epoch 2 Batch_Num 105 Loss 0.005456903483718634\n",
      "Epoch 2 Batch_Num 106 Loss 0.011849269270896912\n",
      "Epoch 2 Batch_Num 107 Loss 0.021296298131346703\n",
      "Epoch 2 Batch_Num 108 Loss 0.01307065598666668\n",
      "Epoch 2 Batch_Num 109 Loss 0.007019811309874058\n",
      "Epoch 2 Batch_Num 110 Loss 0.020579230040311813\n",
      "Epoch 2 Batch_Num 111 Loss 0.01471458189189434\n",
      "Epoch 2 Batch_Num 112 Loss 0.010612835176289082\n",
      "Epoch 2 Batch_Num 113 Loss 0.01158936321735382\n",
      "Epoch 2 Batch_Num 114 Loss 0.013386299833655357\n",
      "Epoch 2 Batch_Num 115 Loss 0.011013850569725037\n",
      "Epoch 2 Batch_Num 116 Loss 0.012054121121764183\n",
      "Epoch 2 Batch_Num 117 Loss 0.01025887206196785\n",
      "Epoch 2 Batch_Num 118 Loss 0.004595858510583639\n",
      "Epoch 2 Batch_Num 119 Loss 0.015104062855243683\n",
      "Epoch 2 Batch_Num 120 Loss 0.009199650958180428\n",
      "Epoch 2 Batch_Num 121 Loss 0.011733981780707836\n",
      "Epoch 2 Batch_Num 122 Loss 0.012643473222851753\n",
      "Epoch 2 Batch_Num 123 Loss 0.012270363047719002\n",
      "Epoch 2 Batch_Num 124 Loss 0.0068810670636594296\n",
      "Epoch 2 Batch_Num 125 Loss 0.011200717650353909\n",
      "Epoch 2 Batch_Num 126 Loss 0.024153592064976692\n",
      "Epoch 2 Batch_Num 127 Loss 0.004170521628111601\n",
      "Epoch 2 Batch_Num 128 Loss 0.01003600750118494\n",
      "Epoch 2 Batch_Num 129 Loss 0.006493339780718088\n",
      "Epoch 2 Batch_Num 130 Loss 0.015784043818712234\n",
      "Epoch 2 Batch_Num 131 Loss 0.008175307884812355\n",
      "Epoch 2 Batch_Num 132 Loss 0.00556305842474103\n",
      "Epoch 2 Batch_Num 133 Loss 0.004533868748694658\n",
      "Epoch 2 Batch_Num 134 Loss 0.018695276230573654\n",
      "Epoch 2 Batch_Num 135 Loss 0.028031717985868454\n",
      "Epoch 2 Batch_Num 136 Loss 0.007550376001745462\n",
      "Epoch 2 Batch_Num 137 Loss 0.018607862293720245\n",
      "Epoch 2 Batch_Num 138 Loss 0.006945003755390644\n",
      "Epoch 2 Batch_Num 139 Loss 0.009937603957951069\n",
      "Epoch 2 Batch_Num 140 Loss 0.015175120905041695\n",
      "Epoch 2 Batch_Num 141 Loss 0.007475266698747873\n",
      "Epoch 2 Batch_Num 142 Loss 0.012034449726343155\n",
      "Epoch 2 Batch_Num 143 Loss 0.011697540059685707\n",
      "Epoch 2 Batch_Num 144 Loss 0.01088534202426672\n",
      "Epoch 2 Batch_Num 145 Loss 0.007152344100177288\n",
      "Epoch 2 Batch_Num 146 Loss 0.01583816483616829\n",
      "Epoch 2 Batch_Num 147 Loss 0.018045414239168167\n",
      "Epoch 2 Batch_Num 148 Loss 0.004773214925080538\n",
      "Epoch 2 Batch_Num 149 Loss 0.009411686100065708\n",
      "Epoch 2 Batch_Num 150 Loss 0.010077422484755516\n",
      "Epoch 2 Batch_Num 151 Loss 0.007576789706945419\n",
      "Epoch 2 Batch_Num 152 Loss 0.004210260231047869\n",
      "Epoch 2 Batch_Num 153 Loss 0.014711444266140461\n",
      "Epoch 2 Batch_Num 154 Loss 0.007599367760121822\n",
      "Epoch 2 Batch_Num 155 Loss 0.012956519611179829\n",
      "Epoch 2 Batch_Num 156 Loss 0.006280218716710806\n",
      "Epoch 2 Batch_Num 157 Loss 0.012971867807209492\n",
      "Epoch 2 Batch_Num 158 Loss 0.008236226625740528\n",
      "Epoch 2 Batch_Num 159 Loss 0.004917168989777565\n",
      "Epoch 2 Batch_Num 160 Loss 0.01109393686056137\n",
      "Epoch 2 Batch_Num 161 Loss 0.024149667471647263\n",
      "Epoch 2 Batch_Num 162 Loss 0.013484485447406769\n",
      "Epoch 2 Batch_Num 163 Loss 0.009315325878560543\n",
      "Epoch 2 Batch_Num 164 Loss 0.010505727492272854\n",
      "Epoch 2 Batch_Num 165 Loss 0.007831307128071785\n",
      "Epoch 2 Batch_Num 166 Loss 0.014313356950879097\n",
      "Epoch 2 Batch_Num 167 Loss 0.015382220968604088\n",
      "Epoch 2 Batch_Num 168 Loss 0.016212986782193184\n",
      "Epoch 2 Batch_Num 169 Loss 0.009387535974383354\n",
      "Epoch 2 Batch_Num 170 Loss 0.01616455800831318\n",
      "Epoch 2 Batch_Num 171 Loss 0.013875151984393597\n",
      "Epoch 2 Batch_Num 172 Loss 0.01250448264181614\n",
      "Epoch 2 Batch_Num 173 Loss 0.016200732439756393\n",
      "Epoch 2 Batch_Num 174 Loss 0.005547248758375645\n",
      "Epoch 2 Batch_Num 175 Loss 0.0058784326538443565\n",
      "Epoch 2 Batch_Num 176 Loss 0.0035837769974023104\n",
      "Epoch 2 Batch_Num 177 Loss 0.01272072084248066\n",
      "Epoch 2 Batch_Num 178 Loss 0.012417922727763653\n",
      "Epoch 2 Batch_Num 179 Loss 0.01932772621512413\n",
      "Epoch 2 Batch_Num 180 Loss 0.014301463961601257\n",
      "Epoch 2 Batch_Num 181 Loss 0.009528366848826408\n",
      "Epoch 2 Batch_Num 182 Loss 0.006846260279417038\n",
      "Epoch 2 Batch_Num 183 Loss 0.013554500415921211\n",
      "Epoch 2 Batch_Num 184 Loss 0.013692776672542095\n",
      "Epoch 2 Batch_Num 185 Loss 0.018316905945539474\n",
      "Epoch 2 Batch_Num 186 Loss 0.013248233124613762\n",
      "Epoch 2 Batch_Num 187 Loss 0.009754392318427563\n",
      "Epoch 2 Batch_Num 188 Loss 0.007117157336324453\n",
      "Epoch 2 Batch_Num 189 Loss 0.003159798216074705\n",
      "Epoch 2 Batch_Num 190 Loss 0.006384816952049732\n",
      "Epoch 2 Batch_Num 191 Loss 0.020653095096349716\n",
      "Epoch 2 Batch_Num 192 Loss 0.009605183266103268\n",
      "Epoch 2 Batch_Num 193 Loss 0.012972826138138771\n",
      "Epoch 2 Batch_Num 194 Loss 0.015782315284013748\n",
      "Epoch 2 Batch_Num 195 Loss 0.005984724499285221\n",
      "Epoch 2 Batch_Num 196 Loss 0.018523450940847397\n",
      "Epoch 2 Batch_Num 197 Loss 0.006921467836946249\n",
      "Epoch 2 Batch_Num 198 Loss 0.008187456987798214\n",
      "Epoch 2 Batch_Num 199 Loss 0.005576737225055695\n",
      "Epoch 2 Batch_Num 200 Loss 0.0071095796301960945\n",
      "Epoch 2 Batch_Num 201 Loss 0.009039917029440403\n",
      "Epoch 2 Batch_Num 202 Loss 0.011267097666859627\n",
      "Epoch 2 Batch_Num 203 Loss 0.022054512053728104\n",
      "Epoch 2 Batch_Num 204 Loss 0.007728212513029575\n",
      "Epoch 2 Batch_Num 205 Loss 0.008335426449775696\n",
      "Epoch 2 Batch_Num 206 Loss 0.027090445160865784\n",
      "Epoch 2 Batch_Num 207 Loss 0.012649117037653923\n",
      "Epoch 2 Batch_Num 208 Loss 0.011765516363084316\n",
      "Epoch 2 Batch_Num 209 Loss 0.030093243345618248\n",
      "Epoch 2 Batch_Num 210 Loss 0.015955623239278793\n",
      "Epoch 2 Batch_Num 211 Loss 0.015396347269415855\n",
      "Epoch 2 Batch_Num 212 Loss 0.0038253148086369038\n",
      "Epoch 2 Batch_Num 213 Loss 0.007806219160556793\n",
      "Epoch 2 Batch_Num 214 Loss 0.008592180907726288\n",
      "Epoch 2 Batch_Num 215 Loss 0.008220886811614037\n",
      "Epoch 2 Batch_Num 216 Loss 0.016371970996260643\n",
      "Epoch 2 Batch_Num 217 Loss 0.013082204386591911\n",
      "Epoch 2 Batch_Num 218 Loss 0.00774780148640275\n",
      "Epoch 2 Batch_Num 219 Loss 0.009693151339888573\n",
      "Epoch 2 Batch_Num 220 Loss 0.008326574228703976\n",
      "Epoch 2 Batch_Num 221 Loss 0.006905253045260906\n",
      "Epoch 2 Batch_Num 222 Loss 0.007183200214058161\n",
      "Epoch 2 Batch_Num 223 Loss 0.010191274806857109\n",
      "Epoch 2 Batch_Num 224 Loss 0.007507035043090582\n",
      "Epoch 2 Batch_Num 225 Loss 0.02199116349220276\n",
      "Epoch 2 Batch_Num 226 Loss 0.010236550122499466\n",
      "Epoch 2 Batch_Num 227 Loss 0.0030614491552114487\n",
      "Epoch 2 Batch_Num 228 Loss 0.0020084301941096783\n",
      "Epoch 2 Batch_Num 229 Loss 0.006126910913735628\n",
      "Epoch 2 Batch_Num 230 Loss 0.0007549970177933574\n",
      "Epoch 2 Batch_Num 231 Loss 0.003768583294004202\n",
      "Epoch 2 Batch_Num 232 Loss 0.003083431627601385\n",
      "Epoch 2 Batch_Num 233 Loss 0.021992938593029976\n",
      "Epoch 2 Batch_Num 234 Loss 0.020710498094558716\n",
      "Epoch 3 Batch_Num 0 Loss 0.017530905082821846\n",
      "Epoch 3 Batch_Num 1 Loss 0.009661084972321987\n",
      "Epoch 3 Batch_Num 2 Loss 0.012255875393748283\n",
      "Epoch 3 Batch_Num 3 Loss 0.013852673582732677\n",
      "Epoch 3 Batch_Num 4 Loss 0.013253333978354931\n",
      "Epoch 3 Batch_Num 5 Loss 0.01729322224855423\n",
      "Epoch 3 Batch_Num 6 Loss 0.011134388856589794\n",
      "Epoch 3 Batch_Num 7 Loss 0.007486408110707998\n",
      "Epoch 3 Batch_Num 8 Loss 0.007295149378478527\n",
      "Epoch 3 Batch_Num 9 Loss 0.0023958291858434677\n",
      "Epoch 3 Batch_Num 10 Loss 0.01310769747942686\n",
      "Epoch 3 Batch_Num 11 Loss 0.006882204208523035\n",
      "Epoch 3 Batch_Num 12 Loss 0.0035835369490087032\n",
      "Epoch 3 Batch_Num 13 Loss 0.009888917207717896\n",
      "Epoch 3 Batch_Num 14 Loss 0.013296815566718578\n",
      "Epoch 3 Batch_Num 15 Loss 0.00341152586042881\n",
      "Epoch 3 Batch_Num 16 Loss 0.005220725201070309\n",
      "Epoch 3 Batch_Num 17 Loss 0.005252056755125523\n",
      "Epoch 3 Batch_Num 18 Loss 0.005221964325755835\n",
      "Epoch 3 Batch_Num 19 Loss 0.015358375385403633\n",
      "Epoch 3 Batch_Num 20 Loss 0.015196194872260094\n",
      "Epoch 3 Batch_Num 21 Loss 0.004008136689662933\n",
      "Epoch 3 Batch_Num 22 Loss 0.014079024083912373\n",
      "Epoch 3 Batch_Num 23 Loss 0.01010763831436634\n",
      "Epoch 3 Batch_Num 24 Loss 0.007996045053005219\n",
      "Epoch 3 Batch_Num 25 Loss 0.007955474779009819\n",
      "Epoch 3 Batch_Num 26 Loss 0.01141354814171791\n",
      "Epoch 3 Batch_Num 27 Loss 0.009865274652838707\n",
      "Epoch 3 Batch_Num 28 Loss 0.011895429342985153\n",
      "Epoch 3 Batch_Num 29 Loss 0.00514055322855711\n",
      "Epoch 3 Batch_Num 30 Loss 0.014443330466747284\n",
      "Epoch 3 Batch_Num 31 Loss 0.009380756877362728\n",
      "Epoch 3 Batch_Num 32 Loss 0.01591632515192032\n",
      "Epoch 3 Batch_Num 33 Loss 0.004422253929078579\n",
      "Epoch 3 Batch_Num 34 Loss 0.014206873252987862\n",
      "Epoch 3 Batch_Num 35 Loss 0.0035483897663652897\n",
      "Epoch 3 Batch_Num 36 Loss 0.013660659082233906\n",
      "Epoch 3 Batch_Num 37 Loss 0.006400226149708033\n",
      "Epoch 3 Batch_Num 38 Loss 0.005610847845673561\n",
      "Epoch 3 Batch_Num 39 Loss 0.008875967934727669\n",
      "Epoch 3 Batch_Num 40 Loss 0.008255851455032825\n",
      "Epoch 3 Batch_Num 41 Loss 0.004653979558497667\n",
      "Epoch 3 Batch_Num 42 Loss 0.01855914667248726\n",
      "Epoch 3 Batch_Num 43 Loss 0.010427910834550858\n",
      "Epoch 3 Batch_Num 44 Loss 0.006090284325182438\n",
      "Epoch 3 Batch_Num 45 Loss 0.018083563074469566\n",
      "Epoch 3 Batch_Num 46 Loss 0.012788976542651653\n",
      "Epoch 3 Batch_Num 47 Loss 0.0048210350796580315\n",
      "Epoch 3 Batch_Num 48 Loss 0.006827869918197393\n",
      "Epoch 3 Batch_Num 49 Loss 0.006696825381368399\n",
      "Epoch 3 Batch_Num 50 Loss 0.00714758038520813\n",
      "Epoch 3 Batch_Num 51 Loss 0.015992380678653717\n",
      "Epoch 3 Batch_Num 52 Loss 0.011095781810581684\n",
      "Epoch 3 Batch_Num 53 Loss 0.011963694356381893\n",
      "Epoch 3 Batch_Num 54 Loss 0.0086668711155653\n",
      "Epoch 3 Batch_Num 55 Loss 0.01061013899743557\n",
      "Epoch 3 Batch_Num 56 Loss 0.013369796797633171\n",
      "Epoch 3 Batch_Num 57 Loss 0.005570557434111834\n",
      "Epoch 3 Batch_Num 58 Loss 0.007050888147205114\n",
      "Epoch 3 Batch_Num 59 Loss 0.01017330028116703\n",
      "Epoch 3 Batch_Num 60 Loss 0.0043792808428406715\n",
      "Epoch 3 Batch_Num 61 Loss 0.014862020500004292\n",
      "Epoch 3 Batch_Num 62 Loss 0.009015503339469433\n",
      "Epoch 3 Batch_Num 63 Loss 0.009995224885642529\n",
      "Epoch 3 Batch_Num 64 Loss 0.011058742180466652\n",
      "Epoch 3 Batch_Num 65 Loss 0.011866450309753418\n",
      "Epoch 3 Batch_Num 66 Loss 0.009434856474399567\n",
      "Epoch 3 Batch_Num 67 Loss 0.009796220809221268\n",
      "Epoch 3 Batch_Num 68 Loss 0.005782085005193949\n",
      "Epoch 3 Batch_Num 69 Loss 0.012077881023287773\n",
      "Epoch 3 Batch_Num 70 Loss 0.007153297774493694\n",
      "Epoch 3 Batch_Num 71 Loss 0.00879861693829298\n",
      "Epoch 3 Batch_Num 72 Loss 0.002909379545599222\n",
      "Epoch 3 Batch_Num 73 Loss 0.0038922287058085203\n",
      "Epoch 3 Batch_Num 74 Loss 0.0064252219162881374\n",
      "Epoch 3 Batch_Num 75 Loss 0.008182656951248646\n",
      "Epoch 3 Batch_Num 76 Loss 0.006714061833918095\n",
      "Epoch 3 Batch_Num 77 Loss 0.010179819539189339\n",
      "Epoch 3 Batch_Num 78 Loss 0.01783173531293869\n",
      "Epoch 3 Batch_Num 79 Loss 0.009994915686547756\n",
      "Epoch 3 Batch_Num 80 Loss 0.011335867457091808\n",
      "Epoch 3 Batch_Num 81 Loss 0.006359625607728958\n",
      "Epoch 3 Batch_Num 82 Loss 0.006926099769771099\n",
      "Epoch 3 Batch_Num 83 Loss 0.0035400018095970154\n",
      "Epoch 3 Batch_Num 84 Loss 0.007774954196065664\n",
      "Epoch 3 Batch_Num 85 Loss 0.004110103473067284\n",
      "Epoch 3 Batch_Num 86 Loss 0.005795066244900227\n",
      "Epoch 3 Batch_Num 87 Loss 0.0035580932162702084\n",
      "Epoch 3 Batch_Num 88 Loss 0.01127634011209011\n",
      "Epoch 3 Batch_Num 89 Loss 0.006152837071567774\n",
      "Epoch 3 Batch_Num 90 Loss 0.006882546003907919\n",
      "Epoch 3 Batch_Num 91 Loss 0.004747670143842697\n",
      "Epoch 3 Batch_Num 92 Loss 0.008640117943286896\n",
      "Epoch 3 Batch_Num 93 Loss 0.01340851653367281\n",
      "Epoch 3 Batch_Num 94 Loss 0.005151562392711639\n",
      "Epoch 3 Batch_Num 95 Loss 0.0028102074284106493\n",
      "Epoch 3 Batch_Num 96 Loss 0.014609253033995628\n",
      "Epoch 3 Batch_Num 97 Loss 0.010029565542936325\n",
      "Epoch 3 Batch_Num 98 Loss 0.010378118604421616\n",
      "Epoch 3 Batch_Num 99 Loss 0.008281045593321323\n",
      "Epoch 3 Batch_Num 100 Loss 0.0048446860164403915\n",
      "Epoch 3 Batch_Num 101 Loss 0.004313579760491848\n",
      "Epoch 3 Batch_Num 102 Loss 0.003689715638756752\n",
      "Epoch 3 Batch_Num 103 Loss 0.020917275920510292\n",
      "Epoch 3 Batch_Num 104 Loss 0.013050323352217674\n",
      "Epoch 3 Batch_Num 105 Loss 0.0067836144007742405\n",
      "Epoch 3 Batch_Num 106 Loss 0.011185416020452976\n",
      "Epoch 3 Batch_Num 107 Loss 0.014088605530560017\n",
      "Epoch 3 Batch_Num 108 Loss 0.009870153851807117\n",
      "Epoch 3 Batch_Num 109 Loss 0.005117302760481834\n",
      "Epoch 3 Batch_Num 110 Loss 0.0176281426101923\n",
      "Epoch 3 Batch_Num 111 Loss 0.01753978803753853\n",
      "Epoch 3 Batch_Num 112 Loss 0.013842150568962097\n",
      "Epoch 3 Batch_Num 113 Loss 0.008152907714247704\n",
      "Epoch 3 Batch_Num 114 Loss 0.009215937927365303\n",
      "Epoch 3 Batch_Num 115 Loss 0.010658561252057552\n",
      "Epoch 3 Batch_Num 116 Loss 0.011705405078828335\n",
      "Epoch 3 Batch_Num 117 Loss 0.012463444843888283\n",
      "Epoch 3 Batch_Num 118 Loss 0.0038752735126763582\n",
      "Epoch 3 Batch_Num 119 Loss 0.012461774982511997\n",
      "Epoch 3 Batch_Num 120 Loss 0.008955269120633602\n",
      "Epoch 3 Batch_Num 121 Loss 0.010282028466463089\n",
      "Epoch 3 Batch_Num 122 Loss 0.013648097403347492\n",
      "Epoch 3 Batch_Num 123 Loss 0.0074189649894833565\n",
      "Epoch 3 Batch_Num 124 Loss 0.00669759651646018\n",
      "Epoch 3 Batch_Num 125 Loss 0.0073058768175542355\n",
      "Epoch 3 Batch_Num 126 Loss 0.022458277642726898\n",
      "Epoch 3 Batch_Num 127 Loss 0.0020684744231402874\n",
      "Epoch 3 Batch_Num 128 Loss 0.007635613437741995\n",
      "Epoch 3 Batch_Num 129 Loss 0.004332124721258879\n",
      "Epoch 3 Batch_Num 130 Loss 0.017404112964868546\n",
      "Epoch 3 Batch_Num 131 Loss 0.005106193013489246\n",
      "Epoch 3 Batch_Num 132 Loss 0.002989160595461726\n",
      "Epoch 3 Batch_Num 133 Loss 0.003067021956667304\n",
      "Epoch 3 Batch_Num 134 Loss 0.011909568682312965\n",
      "Epoch 3 Batch_Num 135 Loss 0.01754828169941902\n",
      "Epoch 3 Batch_Num 136 Loss 0.006226710043847561\n",
      "Epoch 3 Batch_Num 137 Loss 0.016608670353889465\n",
      "Epoch 3 Batch_Num 138 Loss 0.007334481924772263\n",
      "Epoch 3 Batch_Num 139 Loss 0.010195002891123295\n",
      "Epoch 3 Batch_Num 140 Loss 0.011894579976797104\n",
      "Epoch 3 Batch_Num 141 Loss 0.0054387180134654045\n",
      "Epoch 3 Batch_Num 142 Loss 0.008224575780332088\n",
      "Epoch 3 Batch_Num 143 Loss 0.00932002067565918\n",
      "Epoch 3 Batch_Num 144 Loss 0.010114571079611778\n",
      "Epoch 3 Batch_Num 145 Loss 0.008215532638132572\n",
      "Epoch 3 Batch_Num 146 Loss 0.01029048953205347\n",
      "Epoch 3 Batch_Num 147 Loss 0.013592126779258251\n",
      "Epoch 3 Batch_Num 148 Loss 0.002073956187814474\n",
      "Epoch 3 Batch_Num 149 Loss 0.009303675964474678\n",
      "Epoch 3 Batch_Num 150 Loss 0.009711149148643017\n",
      "Epoch 3 Batch_Num 151 Loss 0.00981204118579626\n",
      "Epoch 3 Batch_Num 152 Loss 0.001643021241761744\n",
      "Epoch 3 Batch_Num 153 Loss 0.01152120716869831\n",
      "Epoch 3 Batch_Num 154 Loss 0.005683596711605787\n",
      "Epoch 3 Batch_Num 155 Loss 0.008028799667954445\n",
      "Epoch 3 Batch_Num 156 Loss 0.0065846070647239685\n",
      "Epoch 3 Batch_Num 157 Loss 0.00851691048592329\n",
      "Epoch 3 Batch_Num 158 Loss 0.003112033475190401\n",
      "Epoch 3 Batch_Num 159 Loss 0.004175248555839062\n",
      "Epoch 3 Batch_Num 160 Loss 0.006439125631004572\n",
      "Epoch 3 Batch_Num 161 Loss 0.01657712087035179\n",
      "Epoch 3 Batch_Num 162 Loss 0.01051324512809515\n",
      "Epoch 3 Batch_Num 163 Loss 0.006254854146391153\n",
      "Epoch 3 Batch_Num 164 Loss 0.0052278670482337475\n",
      "Epoch 3 Batch_Num 165 Loss 0.004483300726860762\n",
      "Epoch 3 Batch_Num 166 Loss 0.009719986468553543\n",
      "Epoch 3 Batch_Num 167 Loss 0.013206598348915577\n",
      "Epoch 3 Batch_Num 168 Loss 0.012109843082726002\n",
      "Epoch 3 Batch_Num 169 Loss 0.007380330003798008\n",
      "Epoch 3 Batch_Num 170 Loss 0.007875008508563042\n",
      "Epoch 3 Batch_Num 171 Loss 0.009786410257220268\n",
      "Epoch 3 Batch_Num 172 Loss 0.007716276682913303\n",
      "Epoch 3 Batch_Num 173 Loss 0.01599380373954773\n",
      "Epoch 3 Batch_Num 174 Loss 0.006018493324518204\n",
      "Epoch 3 Batch_Num 175 Loss 0.0032577719539403915\n",
      "Epoch 3 Batch_Num 176 Loss 0.0025551533326506615\n",
      "Epoch 3 Batch_Num 177 Loss 0.008059524931013584\n",
      "Epoch 3 Batch_Num 178 Loss 0.005718333646655083\n",
      "Epoch 3 Batch_Num 179 Loss 0.011266103014349937\n",
      "Epoch 3 Batch_Num 180 Loss 0.01256356667727232\n",
      "Epoch 3 Batch_Num 181 Loss 0.007478785701096058\n",
      "Epoch 3 Batch_Num 182 Loss 0.006894613150507212\n",
      "Epoch 3 Batch_Num 183 Loss 0.01090959645807743\n",
      "Epoch 3 Batch_Num 184 Loss 0.008978737518191338\n",
      "Epoch 3 Batch_Num 185 Loss 0.016357960179448128\n",
      "Epoch 3 Batch_Num 186 Loss 0.004181623924523592\n",
      "Epoch 3 Batch_Num 187 Loss 0.005904483143240213\n",
      "Epoch 3 Batch_Num 188 Loss 0.004563729744404554\n",
      "Epoch 3 Batch_Num 189 Loss 0.004256126936525106\n",
      "Epoch 3 Batch_Num 190 Loss 0.0072159962728619576\n",
      "Epoch 3 Batch_Num 191 Loss 0.01486834418028593\n",
      "Epoch 3 Batch_Num 192 Loss 0.00963000487536192\n",
      "Epoch 3 Batch_Num 193 Loss 0.007772549986839294\n",
      "Epoch 3 Batch_Num 194 Loss 0.013802984729409218\n",
      "Epoch 3 Batch_Num 195 Loss 0.004779682960361242\n",
      "Epoch 3 Batch_Num 196 Loss 0.015728190541267395\n",
      "Epoch 3 Batch_Num 197 Loss 0.004015416372567415\n",
      "Epoch 3 Batch_Num 198 Loss 0.006540436297655106\n",
      "Epoch 3 Batch_Num 199 Loss 0.0035826615057885647\n",
      "Epoch 3 Batch_Num 200 Loss 0.00704478332772851\n",
      "Epoch 3 Batch_Num 201 Loss 0.007732678204774857\n",
      "Epoch 3 Batch_Num 202 Loss 0.012310592457652092\n",
      "Epoch 3 Batch_Num 203 Loss 0.010627537965774536\n",
      "Epoch 3 Batch_Num 204 Loss 0.007996977306902409\n",
      "Epoch 3 Batch_Num 205 Loss 0.008330196142196655\n",
      "Epoch 3 Batch_Num 206 Loss 0.018133405596017838\n",
      "Epoch 3 Batch_Num 207 Loss 0.008251023478806019\n",
      "Epoch 3 Batch_Num 208 Loss 0.013082128949463367\n",
      "Epoch 3 Batch_Num 209 Loss 0.008044700138270855\n",
      "Epoch 3 Batch_Num 210 Loss 0.008287280797958374\n",
      "Epoch 3 Batch_Num 211 Loss 0.009799832478165627\n",
      "Epoch 3 Batch_Num 212 Loss 0.003670957637950778\n",
      "Epoch 3 Batch_Num 213 Loss 0.004133991897106171\n",
      "Epoch 3 Batch_Num 214 Loss 0.004422079771757126\n",
      "Epoch 3 Batch_Num 215 Loss 0.0031086888629943132\n",
      "Epoch 3 Batch_Num 216 Loss 0.009006451815366745\n",
      "Epoch 3 Batch_Num 217 Loss 0.0071530407294631\n",
      "Epoch 3 Batch_Num 218 Loss 0.005994540173560381\n",
      "Epoch 3 Batch_Num 219 Loss 0.007789246737957001\n",
      "Epoch 3 Batch_Num 220 Loss 0.004450381267815828\n",
      "Epoch 3 Batch_Num 221 Loss 0.004831729922443628\n",
      "Epoch 3 Batch_Num 222 Loss 0.008036279119551182\n",
      "Epoch 3 Batch_Num 223 Loss 0.004141231067478657\n",
      "Epoch 3 Batch_Num 224 Loss 0.002497172448784113\n",
      "Epoch 3 Batch_Num 225 Loss 0.015215901657938957\n",
      "Epoch 3 Batch_Num 226 Loss 0.006872369907796383\n",
      "Epoch 3 Batch_Num 227 Loss 0.0009211081778630614\n",
      "Epoch 3 Batch_Num 228 Loss 0.005777041427791119\n",
      "Epoch 3 Batch_Num 229 Loss 0.005295249633491039\n",
      "Epoch 3 Batch_Num 230 Loss 0.0005746508250012994\n",
      "Epoch 3 Batch_Num 231 Loss 0.007068348117172718\n",
      "Epoch 3 Batch_Num 232 Loss 0.007332792039960623\n",
      "Epoch 3 Batch_Num 233 Loss 0.02175762876868248\n",
      "Epoch 3 Batch_Num 234 Loss 0.012961865402758121\n",
      "Epoch 4 Batch_Num 0 Loss 0.019379347562789917\n",
      "Epoch 4 Batch_Num 1 Loss 0.007615440990775824\n",
      "Epoch 4 Batch_Num 2 Loss 0.009474994614720345\n",
      "Epoch 4 Batch_Num 3 Loss 0.01195908896625042\n",
      "Epoch 4 Batch_Num 4 Loss 0.011377978138625622\n",
      "Epoch 4 Batch_Num 5 Loss 0.015034342184662819\n",
      "Epoch 4 Batch_Num 6 Loss 0.010414239019155502\n",
      "Epoch 4 Batch_Num 7 Loss 0.004742334596812725\n",
      "Epoch 4 Batch_Num 8 Loss 0.0044859666377305984\n",
      "Epoch 4 Batch_Num 9 Loss 0.0037472837138921022\n",
      "Epoch 4 Batch_Num 10 Loss 0.010654100216925144\n",
      "Epoch 4 Batch_Num 11 Loss 0.005036134272813797\n",
      "Epoch 4 Batch_Num 12 Loss 0.002938617020845413\n",
      "Epoch 4 Batch_Num 13 Loss 0.012795965187251568\n",
      "Epoch 4 Batch_Num 14 Loss 0.01525689847767353\n",
      "Epoch 4 Batch_Num 15 Loss 0.004805839154869318\n",
      "Epoch 4 Batch_Num 16 Loss 0.00291870953515172\n",
      "Epoch 4 Batch_Num 17 Loss 0.010323735885322094\n",
      "Epoch 4 Batch_Num 18 Loss 0.004148764535784721\n",
      "Epoch 4 Batch_Num 19 Loss 0.014231076464056969\n",
      "Epoch 4 Batch_Num 20 Loss 0.01351204328238964\n",
      "Epoch 4 Batch_Num 21 Loss 0.004599868319928646\n",
      "Epoch 4 Batch_Num 22 Loss 0.01054003369063139\n",
      "Epoch 4 Batch_Num 23 Loss 0.00705734035000205\n",
      "Epoch 4 Batch_Num 24 Loss 0.007068322040140629\n",
      "Epoch 4 Batch_Num 25 Loss 0.004562120418995619\n",
      "Epoch 4 Batch_Num 26 Loss 0.007735344581305981\n",
      "Epoch 4 Batch_Num 27 Loss 0.00792944710701704\n",
      "Epoch 4 Batch_Num 28 Loss 0.00981147401034832\n",
      "Epoch 4 Batch_Num 29 Loss 0.003232726827263832\n",
      "Epoch 4 Batch_Num 30 Loss 0.009116551838815212\n",
      "Epoch 4 Batch_Num 31 Loss 0.005741097033023834\n",
      "Epoch 4 Batch_Num 32 Loss 0.014054736122488976\n",
      "Epoch 4 Batch_Num 33 Loss 0.004901403095573187\n",
      "Epoch 4 Batch_Num 34 Loss 0.015517639927566051\n",
      "Epoch 4 Batch_Num 35 Loss 0.005066784098744392\n",
      "Epoch 4 Batch_Num 36 Loss 0.007589268498122692\n",
      "Epoch 4 Batch_Num 37 Loss 0.003766212612390518\n",
      "Epoch 4 Batch_Num 38 Loss 0.005451495759189129\n",
      "Epoch 4 Batch_Num 39 Loss 0.012947812676429749\n",
      "Epoch 4 Batch_Num 40 Loss 0.010971049778163433\n",
      "Epoch 4 Batch_Num 41 Loss 0.0012850540224462748\n",
      "Epoch 4 Batch_Num 42 Loss 0.0184539295732975\n",
      "Epoch 4 Batch_Num 43 Loss 0.006415969226509333\n",
      "Epoch 4 Batch_Num 44 Loss 0.002677186857908964\n",
      "Epoch 4 Batch_Num 45 Loss 0.011918219737708569\n",
      "Epoch 4 Batch_Num 46 Loss 0.008995898999273777\n",
      "Epoch 4 Batch_Num 47 Loss 0.004000483080744743\n",
      "Epoch 4 Batch_Num 48 Loss 0.007123177405446768\n",
      "Epoch 4 Batch_Num 49 Loss 0.007748103700578213\n",
      "Epoch 4 Batch_Num 50 Loss 0.010040130466222763\n",
      "Epoch 4 Batch_Num 51 Loss 0.009220779873430729\n",
      "Epoch 4 Batch_Num 52 Loss 0.0054732938297092915\n",
      "Epoch 4 Batch_Num 53 Loss 0.008857978507876396\n",
      "Epoch 4 Batch_Num 54 Loss 0.007076100911945105\n",
      "Epoch 4 Batch_Num 55 Loss 0.008558044210076332\n",
      "Epoch 4 Batch_Num 56 Loss 0.01309039257466793\n",
      "Epoch 4 Batch_Num 57 Loss 0.00664022471755743\n",
      "Epoch 4 Batch_Num 58 Loss 0.006586678326129913\n",
      "Epoch 4 Batch_Num 59 Loss 0.007319048047065735\n",
      "Epoch 4 Batch_Num 60 Loss 0.0036638732999563217\n",
      "Epoch 4 Batch_Num 61 Loss 0.0067152781412005424\n",
      "Epoch 4 Batch_Num 62 Loss 0.008363204076886177\n",
      "Epoch 4 Batch_Num 63 Loss 0.007775482721626759\n",
      "Epoch 4 Batch_Num 64 Loss 0.011561950668692589\n",
      "Epoch 4 Batch_Num 65 Loss 0.007474662270396948\n",
      "Epoch 4 Batch_Num 66 Loss 0.007217890582978725\n",
      "Epoch 4 Batch_Num 67 Loss 0.008954674936830997\n",
      "Epoch 4 Batch_Num 68 Loss 0.006406414322555065\n",
      "Epoch 4 Batch_Num 69 Loss 0.006266546435654163\n",
      "Epoch 4 Batch_Num 70 Loss 0.004578439984470606\n",
      "Epoch 4 Batch_Num 71 Loss 0.007144497241824865\n",
      "Epoch 4 Batch_Num 72 Loss 0.006469520274549723\n",
      "Epoch 4 Batch_Num 73 Loss 0.006200025789439678\n",
      "Epoch 4 Batch_Num 74 Loss 0.006536400411278009\n",
      "Epoch 4 Batch_Num 75 Loss 0.011571057140827179\n",
      "Epoch 4 Batch_Num 76 Loss 0.005355323664844036\n",
      "Epoch 4 Batch_Num 77 Loss 0.007126021198928356\n",
      "Epoch 4 Batch_Num 78 Loss 0.01736120507121086\n",
      "Epoch 4 Batch_Num 79 Loss 0.008174761198461056\n",
      "Epoch 4 Batch_Num 80 Loss 0.010882304981350899\n",
      "Epoch 4 Batch_Num 81 Loss 0.005828569643199444\n",
      "Epoch 4 Batch_Num 82 Loss 0.005808134097605944\n",
      "Epoch 4 Batch_Num 83 Loss 0.0031024240888655186\n",
      "Epoch 4 Batch_Num 84 Loss 0.00900919921696186\n",
      "Epoch 4 Batch_Num 85 Loss 0.0035232093650847673\n",
      "Epoch 4 Batch_Num 86 Loss 0.005047373939305544\n",
      "Epoch 4 Batch_Num 87 Loss 0.006833504885435104\n",
      "Epoch 4 Batch_Num 88 Loss 0.008982340805232525\n",
      "Epoch 4 Batch_Num 89 Loss 0.0018592268461361527\n",
      "Epoch 4 Batch_Num 90 Loss 0.004415787756443024\n",
      "Epoch 4 Batch_Num 91 Loss 0.003943419549614191\n",
      "Epoch 4 Batch_Num 92 Loss 0.007022927049547434\n",
      "Epoch 4 Batch_Num 93 Loss 0.011375470086932182\n",
      "Epoch 4 Batch_Num 94 Loss 0.004698251374065876\n",
      "Epoch 4 Batch_Num 95 Loss 0.0018368482124060392\n",
      "Epoch 4 Batch_Num 96 Loss 0.011639500968158245\n",
      "Epoch 4 Batch_Num 97 Loss 0.0074392044916749\n",
      "Epoch 4 Batch_Num 98 Loss 0.008926932699978352\n",
      "Epoch 4 Batch_Num 99 Loss 0.007219117134809494\n",
      "Epoch 4 Batch_Num 100 Loss 0.006816264241933823\n",
      "Epoch 4 Batch_Num 101 Loss 0.0038272514939308167\n",
      "Epoch 4 Batch_Num 102 Loss 0.004446013830602169\n",
      "Epoch 4 Batch_Num 103 Loss 0.01830935664474964\n",
      "Epoch 4 Batch_Num 104 Loss 0.008159764111042023\n",
      "Epoch 4 Batch_Num 105 Loss 0.009526182897388935\n",
      "Epoch 4 Batch_Num 106 Loss 0.005957198329269886\n",
      "Epoch 4 Batch_Num 107 Loss 0.008626067079603672\n",
      "Epoch 4 Batch_Num 108 Loss 0.005064109805971384\n",
      "Epoch 4 Batch_Num 109 Loss 0.004713826347142458\n",
      "Epoch 4 Batch_Num 110 Loss 0.009369833394885063\n",
      "Epoch 4 Batch_Num 111 Loss 0.00928665790706873\n",
      "Epoch 4 Batch_Num 112 Loss 0.010200299322605133\n",
      "Epoch 4 Batch_Num 113 Loss 0.009218031540513039\n",
      "Epoch 4 Batch_Num 114 Loss 0.006929905153810978\n",
      "Epoch 4 Batch_Num 115 Loss 0.008810343220829964\n",
      "Epoch 4 Batch_Num 116 Loss 0.005562768317759037\n",
      "Epoch 4 Batch_Num 117 Loss 0.00893118791282177\n",
      "Epoch 4 Batch_Num 118 Loss 0.0016404141206294298\n",
      "Epoch 4 Batch_Num 119 Loss 0.00870348047465086\n",
      "Epoch 4 Batch_Num 120 Loss 0.00901186652481556\n",
      "Epoch 4 Batch_Num 121 Loss 0.007678125984966755\n",
      "Epoch 4 Batch_Num 122 Loss 0.009691057726740837\n",
      "Epoch 4 Batch_Num 123 Loss 0.00823290180414915\n",
      "Epoch 4 Batch_Num 124 Loss 0.006037120707333088\n",
      "Epoch 4 Batch_Num 125 Loss 0.006727203726768494\n",
      "Epoch 4 Batch_Num 126 Loss 0.011526582762598991\n",
      "Epoch 4 Batch_Num 127 Loss 0.002305274363607168\n",
      "Epoch 4 Batch_Num 128 Loss 0.008226262405514717\n",
      "Epoch 4 Batch_Num 129 Loss 0.004018140025436878\n",
      "Epoch 4 Batch_Num 130 Loss 0.011196443811058998\n",
      "Epoch 4 Batch_Num 131 Loss 0.00444031460210681\n",
      "Epoch 4 Batch_Num 132 Loss 0.0032189630437642336\n",
      "Epoch 4 Batch_Num 133 Loss 0.0013797509018331766\n",
      "Epoch 4 Batch_Num 134 Loss 0.012554493732750416\n",
      "Epoch 4 Batch_Num 135 Loss 0.007625923492014408\n",
      "Epoch 4 Batch_Num 136 Loss 0.00735024968162179\n",
      "Epoch 4 Batch_Num 137 Loss 0.012357010506093502\n",
      "Epoch 4 Batch_Num 138 Loss 0.006198382005095482\n",
      "Epoch 4 Batch_Num 139 Loss 0.010575281456112862\n",
      "Epoch 4 Batch_Num 140 Loss 0.007668878883123398\n",
      "Epoch 4 Batch_Num 141 Loss 0.0034541289787739515\n",
      "Epoch 4 Batch_Num 142 Loss 0.008654139935970306\n",
      "Epoch 4 Batch_Num 143 Loss 0.007732179947197437\n",
      "Epoch 4 Batch_Num 144 Loss 0.006447174586355686\n",
      "Epoch 4 Batch_Num 145 Loss 0.005637123249471188\n",
      "Epoch 4 Batch_Num 146 Loss 0.010037827305495739\n",
      "Epoch 4 Batch_Num 147 Loss 0.00932303536683321\n",
      "Epoch 4 Batch_Num 148 Loss 0.0012296841014176607\n",
      "Epoch 4 Batch_Num 149 Loss 0.007027020212262869\n",
      "Epoch 4 Batch_Num 150 Loss 0.005158252082765102\n",
      "Epoch 4 Batch_Num 151 Loss 0.006087243091315031\n",
      "Epoch 4 Batch_Num 152 Loss 0.0010825321078300476\n",
      "Epoch 4 Batch_Num 153 Loss 0.01350584626197815\n",
      "Epoch 4 Batch_Num 154 Loss 0.005744893103837967\n",
      "Epoch 4 Batch_Num 155 Loss 0.0064910599030554295\n",
      "Epoch 4 Batch_Num 156 Loss 0.004790028091520071\n",
      "Epoch 4 Batch_Num 157 Loss 0.008105004206299782\n",
      "Epoch 4 Batch_Num 158 Loss 0.0027006142772734165\n",
      "Epoch 4 Batch_Num 159 Loss 0.003825586289167404\n",
      "Epoch 4 Batch_Num 160 Loss 0.008200251497328281\n",
      "Epoch 4 Batch_Num 161 Loss 0.007221260108053684\n",
      "Epoch 4 Batch_Num 162 Loss 0.0064171478152275085\n",
      "Epoch 4 Batch_Num 163 Loss 0.005264914128929377\n",
      "Epoch 4 Batch_Num 164 Loss 0.0024422870483249426\n",
      "Epoch 4 Batch_Num 165 Loss 0.0032067683059722185\n",
      "Epoch 4 Batch_Num 166 Loss 0.011263514868915081\n",
      "Epoch 4 Batch_Num 167 Loss 0.008815358392894268\n",
      "Epoch 4 Batch_Num 168 Loss 0.017300207167863846\n",
      "Epoch 4 Batch_Num 169 Loss 0.00361956050619483\n",
      "Epoch 4 Batch_Num 170 Loss 0.010244006291031837\n",
      "Epoch 4 Batch_Num 171 Loss 0.0068733408115804195\n",
      "Epoch 4 Batch_Num 172 Loss 0.006369683891534805\n",
      "Epoch 4 Batch_Num 173 Loss 0.013024451211094856\n",
      "Epoch 4 Batch_Num 174 Loss 0.006456448696553707\n",
      "Epoch 4 Batch_Num 175 Loss 0.00581168569624424\n",
      "Epoch 4 Batch_Num 176 Loss 0.004066402092576027\n",
      "Epoch 4 Batch_Num 177 Loss 0.011092795059084892\n",
      "Epoch 4 Batch_Num 178 Loss 0.004022262524813414\n",
      "Epoch 4 Batch_Num 179 Loss 0.013143369928002357\n",
      "Epoch 4 Batch_Num 180 Loss 0.010015965439379215\n",
      "Epoch 4 Batch_Num 181 Loss 0.005034853704273701\n",
      "Epoch 4 Batch_Num 182 Loss 0.0030969681683927774\n",
      "Epoch 4 Batch_Num 183 Loss 0.00933864712715149\n",
      "Epoch 4 Batch_Num 184 Loss 0.010190604254603386\n",
      "Epoch 4 Batch_Num 185 Loss 0.01125467661768198\n",
      "Epoch 4 Batch_Num 186 Loss 0.0075357286259531975\n",
      "Epoch 4 Batch_Num 187 Loss 0.00603332556784153\n",
      "Epoch 4 Batch_Num 188 Loss 0.004331090021878481\n",
      "Epoch 4 Batch_Num 189 Loss 0.0022482038475573063\n",
      "Epoch 4 Batch_Num 190 Loss 0.002009419957175851\n",
      "Epoch 4 Batch_Num 191 Loss 0.013086278922855854\n",
      "Epoch 4 Batch_Num 192 Loss 0.010426166467368603\n",
      "Epoch 4 Batch_Num 193 Loss 0.005262353457510471\n",
      "Epoch 4 Batch_Num 194 Loss 0.011624250560998917\n",
      "Epoch 4 Batch_Num 195 Loss 0.0055661448277533054\n",
      "Epoch 4 Batch_Num 196 Loss 0.012258579023182392\n",
      "Epoch 4 Batch_Num 197 Loss 0.00292319362051785\n",
      "Epoch 4 Batch_Num 198 Loss 0.007611413951963186\n",
      "Epoch 4 Batch_Num 199 Loss 0.002535254927352071\n",
      "Epoch 4 Batch_Num 200 Loss 0.00509314751252532\n",
      "Epoch 4 Batch_Num 201 Loss 0.006879841443151236\n",
      "Epoch 4 Batch_Num 202 Loss 0.010282618924975395\n",
      "Epoch 4 Batch_Num 203 Loss 0.008990992791950703\n",
      "Epoch 4 Batch_Num 204 Loss 0.010358737781643867\n",
      "Epoch 4 Batch_Num 205 Loss 0.010905852541327477\n",
      "Epoch 4 Batch_Num 206 Loss 0.01577274687588215\n",
      "Epoch 4 Batch_Num 207 Loss 0.00909780990332365\n",
      "Epoch 4 Batch_Num 208 Loss 0.0060410527512431145\n",
      "Epoch 4 Batch_Num 209 Loss 0.007296451833099127\n",
      "Epoch 4 Batch_Num 210 Loss 0.007430561818182468\n",
      "Epoch 4 Batch_Num 211 Loss 0.008859187364578247\n",
      "Epoch 4 Batch_Num 212 Loss 0.006410109810531139\n",
      "Epoch 4 Batch_Num 213 Loss 0.0031089340336620808\n",
      "Epoch 4 Batch_Num 214 Loss 0.0057807909324765205\n",
      "Epoch 4 Batch_Num 215 Loss 0.004431684501469135\n",
      "Epoch 4 Batch_Num 216 Loss 0.009465072304010391\n",
      "Epoch 4 Batch_Num 217 Loss 0.007370516657829285\n",
      "Epoch 4 Batch_Num 218 Loss 0.007644633762538433\n",
      "Epoch 4 Batch_Num 219 Loss 0.006613692734390497\n",
      "Epoch 4 Batch_Num 220 Loss 0.004062415100634098\n",
      "Epoch 4 Batch_Num 221 Loss 0.008974265307188034\n",
      "Epoch 4 Batch_Num 222 Loss 0.00654071057215333\n",
      "Epoch 4 Batch_Num 223 Loss 0.005604908335953951\n",
      "Epoch 4 Batch_Num 224 Loss 0.002791792619973421\n",
      "Epoch 4 Batch_Num 225 Loss 0.007924768142402172\n",
      "Epoch 4 Batch_Num 226 Loss 0.010159974917769432\n",
      "Epoch 4 Batch_Num 227 Loss 0.00012213396257720888\n",
      "Epoch 4 Batch_Num 228 Loss 0.004311398137360811\n",
      "Epoch 4 Batch_Num 229 Loss 0.0020257015712559223\n",
      "Epoch 4 Batch_Num 230 Loss 0.00039070239290595055\n",
      "Epoch 4 Batch_Num 231 Loss 0.0008541237330064178\n",
      "Epoch 4 Batch_Num 232 Loss 0.003327256767079234\n",
      "Epoch 4 Batch_Num 233 Loss 0.016955280676484108\n",
      "Epoch 4 Batch_Num 234 Loss 0.011241033673286438\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for f in range(num_epochs):\n",
    "    for batch_num, minibatch in enumerate(train_loader):\n",
    "        minibatch_x, minibatch_y = minibatch[0], minibatch[1]\n",
    "\n",
    "        # cuda\n",
    "        # output = model.forward(torch.Tensor(minibatch_x.float()).cuda())\n",
    "        # loss = criterion(output, torch.Tensor(minibatch_y.float()).cuda())\n",
    "\n",
    "        # cpu\n",
    "        output = model.forward(torch.Tensor(minibatch_x.float()))\n",
    "        loss = criterion(output, torch.Tensor(minibatch_y.float()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {f} Batch_Num {batch_num} Loss {loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/03/11 14:20:11 INFO mlflow.tracking.fluent: Experiment with name 'PyTorch_MNIST' does not exist. Creating a new experiment.\n",
      "2022/03/11 14:20:11 WARNING mlflow.tracking.context.git_context: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "C:\\Users\\love9\\anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_acc : 0.9766\n",
      "auc_score : 0.9871565095209144\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"PyTorch_MNIST\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    preds = model.forward(torch.Tensor(x_test.float()))\n",
    "    preds = np.round(preds.detach().cpu().numpy())\n",
    "\n",
    "    eval_acc = accuracy_score(y_test, preds)\n",
    "    auc_score = roc_auc_score(y_test, preds)\n",
    "\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.log_metric(\"auc_score\", auc_score)\n",
    "\n",
    "    print(\"eval_acc :\", eval_acc)\n",
    "    print(\"auc_score :\", auc_score)\n",
    "\n",
    "    mlflow.pytorch.log_model(model, \"PyTorch_MNIST\")\n",
    "\n",
    "mlflow.end_run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pytorch.load_model(\"runs:/920372197e7e4d149e994bbe1db6f269/PyTorch_MNIST\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_acc : 0.9766\n",
      "auc_score : 0.9871565095209144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\love9\\anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "preds = loaded_model.forward(torch.Tensor(x_test.float()))\n",
    "preds = np.round(preds.detach().cpu().numpy())\n",
    "eval_acc = accuracy_score(y_test, preds)\n",
    "auc_score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print(\"eval_acc :\", eval_acc)\n",
    "print(\"auc_score :\", auc_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}